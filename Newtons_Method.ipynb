{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Newton's Optimization Method",
   "metadata": {
    "cell_id": "6c53213c68b24afa9c56b967fb830d9e",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Analytical Solutions**. Analytical Solution to the Second Order Derivative (Elements of the Hessian Matrix)\n\n![title](figures/Analytical_Hessian.png)\n\n",
   "metadata": {
    "cell_id": "fbdaa0877fc04f8cbb849308b110cfb8",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 114.78125
   }
  },
  {
   "cell_type": "markdown",
   "source": "Creating a Logistic Regression Class based on Newtons Method.\n\n![title](figures/LinearSystem.png)\n\nUsing the derivations from Above, we can now implement the logistic regression model with Newtons Method.",
   "metadata": {
    "cell_id": "ce4f4043561948b2a434fd1b1d0846d5",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 137.171875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b967d67b14db4e9399ca2f976f814aee",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7958e447",
    "execution_start": 1649363814016,
    "execution_millis": 60,
    "owner_user_id": "55d2aa6f-5941-45c1-992b-c196ae6c1aa6",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 3663
   },
   "source": "#Importing Needed Libaries\nimport numpy as np\nnp.random.seed(50)\n\n#Logistic Regression Class\nclass LogisticRegression:\n\n    def __init__(self, x, y):\n\n        self.x = x #Input Random Data Matrix\n        self.y = y #Input Random Data Target Vector\n        self.beta_vector = self.initialize_betas() #Randomly Initializing Betas\n        self.gradient_vector = self.gradient() #Analytically Solving for the Gradient Vector As an Extra Option\n        self.hessian_matrix = self.hessian() #Analytically Solving for the Hessian Vector as an Extra Option\n        self.optimized_betas = self.netwons_method() #Fit Method to optimize betas (note: not like SKlearn)\n        self.predictions = self.predict() #Making Predictions with the optimized betas\n        self.accuracy = self.get_accuracy() #Getting model accuracy\n    \n    def initialize_betas(self):\n\n        beta_vector = np.random.random(len(self.x[0])) #Randomly Initializing The Beta Vector\n        return beta_vector\n\n    #Analytically Solving for the Gradient\n    def gradient(self):\n\n        gradient_vector = []\n        gradient_value = 0\n\n        for j in range(len(self.beta_vector)): #Finding First order partials for each beta\n\n            for k in range(len(self.y)): #Summing through each data instance\n\n                #First Order equation from above\n                term_one = self.x[k][j] * y[k]\n                term_two_numerator = -1 * (self.x[k][j] * np.exp(np.dot(self.x[k].T, self.beta_vector)))\n                term_two_denominator = 1 + np.exp(np.dot(self.x[k].T, self.beta_vector))\n\n                gradient_value += term_one + (term_two_numerator/term_two_denominator)\n\n            gradient_vector.append(gradient_value)\n            gradient_value = 0\n        \n        return np.array(gradient_vector) #Returning the gradient vector\n\n    #Analytically Solving for the Hessian\n    def hessian(self):\n\n        hessian_matrix = []\n        hessian_value = 0\n\n        for j in range(len(self.beta_vector)): #Iterating through every beta for one pass\n\n            for i in range(len(self.beta_vector)): #Iterating through every other beta for a second pass to build the Hessian Matrix\n\n                for k in range(len(self.y)): #Iterating over every data instance \n\n                    #Hessian Second Order partial computations from above\n                    numerator_term_one = 1 + np.exp(np.dot(self.x[k].T, self.beta_vector))\n                    numerator_term_two = self.x[k][j] * self.x[k][i] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n                    numerator_term_three = self.x[k][j] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n                    numerator_term_four = self.x[k][i] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n                    numerator = (numerator_term_one * numerator_term_two) - (numerator_term_three * numerator_term_four)\n                    \n                    denominator = np.square(1 + np.exp(np.dot(self.x[k].T, self.beta_vector)))\n\n                    hessian_value += numerator/denominator\n                \n                hessian_matrix.append(-1 * hessian_value)\n                hessian_value = 0\n        \n        hessian_matrix = np.array(hessian_matrix)\n        hessian_matrix = hessian_matrix.reshape([len(self.x[0]), len(self.x[0])]) #Reshaping the array to matrix form to finalize the hessian\n        return hessian_matrix\n\n\n    #Find difference of betas for convergence\n    def abs_diff_betas(self, beta_step, current_beta):\n        abs_diff_average = 0\n\n        for i in range(len(beta_step)):\n            abs_diff_average += np.abs(beta_step[i] - current_beta[i])\n        \n        return abs_diff_average / len(self.y)\n\n\n    #Iterative Gradient Solver --> For newtons method\n    def iterative_gradient_solver(self, beta_vect, index):\n\n        first_order_beta = 0\n\n        #Finding a first order partial from the loglikelihood function\n        for k in range(len(self.y)):\n\n            term_one = self.x[k][index] * y[k]\n            term_two_numerator = -1 * (self.x[k][index] * np.exp(np.dot(self.x[k].T, beta_vect)))\n            term_two_denominator = 1 + np.exp(np.dot(self.x[k].T, beta_vect))\n\n            first_order_beta += (term_one + (term_two_numerator/term_two_denominator)) \n    \n        return first_order_beta #Returns the first order partial (element in a gradient)\n\n    #Iterative Hessian Solver\n    def iterative_hessian_solver(self, beta_vect, index):\n\n        second_order_beta = 0\n\n        #Finding second order partial for a given beta (index is used to retrieve the beta)\n        for k in range(len(self.y)):\n\n            numerator_term_one = 1 + np.exp(np.dot(self.x[k].T, self.beta_vector))\n            numerator_term_two = self.x[k][index] * self.x[k][index] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n            numerator_term_three = self.x[k][index] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n            numerator_term_four = self.x[k][index] * np.exp(np.dot(self.x[k].T, self.beta_vector))\n            numerator = (numerator_term_one * numerator_term_two) - (numerator_term_three * numerator_term_four)\n            denominator = np.square(1 + np.exp(np.dot(self.x[k].T, beta_vect)))\n\n            second_order_beta += numerator/denominator\n        \n        return -1 * second_order_beta #Returns the second order partial (element in a hessian)\n\n    \n    #Newtons Method Functions\n\n    def newtons_method_computation(self, index, beta_vect):\n\n        #Computing newtons method as stated above\n        beta_step = beta_vect[index] - (self.iterative_gradient_solver(beta_vect, index)/self.iterative_hessian_solver(beta_vect, index))\n        return beta_step\n    \n\n    def newton_convergence(self, beta_step, current_beta):\n\n        #Checking for convergence (betas do not change very much)\n        flag = True\n        for i in range(len(beta_step)):\n\n            if(np.abs(beta_step[i] - current_beta[i]) > 1e-6):\n                flag = False\n                break\n        \n        return flag\n    \n\n    #Fit method to optimize betas\n    def netwons_method(self):\n\n        beta_steps = [] #Updated Beta vector\n        beta_current = self.beta_vector\n\n        num_iter = 0\n        converged = False\n\n        print(\"_Newtons Method Optimization_\\n\")\n        print(\"Optimizing...\")\n        while(converged != True and num_iter < 30000): #Optimization\n\n            for i in range(len(self.beta_vector)):\n\n                beta_steps.append(self.newtons_method_computation(i,beta_current)) #Update Rule\n            \n            converged = self.newton_convergence(beta_steps, beta_current) #Check convergence\n            num_iter += 1\n            beta_current = beta_steps\n            beta_steps = []\n    \n        if(converged):\n            print(\"======================================================\")\n            print(\"Iteration:\", num_iter)\n            print(\"Average Absolute Beta-Step Difference:\", self.abs_diff_betas(beta_steps, beta_current))\n            print(\"Converge: True\")\n            print(\"\\n_Optimization Terminated Successfully_\")\n            print(\"_Optimized Betas_\", beta_current)\n        \n        else:\n            print(\"_Failed to Converge_\")\n        return beta_current\n\n    def predict(self):\n\n        predictions = []\n        predictions_vector = np.dot(self.optimized_betas, self.x.T) #Using optimized betas to predict\n\n        for i in range(len(predictions_vector)):\n            if(predictions_vector[i] >= 0.5): #Setting the classification constraint\n                predictions.append(1)\n            else:\n                predictions.append(0)\n        \n        return predictions #Returns a list of predictions for the input data (in this case just the training data)\n\n    def get_accuracy(self): #Gets accuracy of the predictions\n\n        correct_classifications = 0\n        for i in range(len(self.predictions)):\n            if(self.predictions[i] == self.y[i]):\n                correct_classifications+=1\n        \n        return 100 * correct_classifications/len(self.y) #Returns the accuracy percentage\n",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1da405b7eb214201895aa08b31fb9868",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "49ae0187",
    "execution_start": 1649363814084,
    "execution_millis": 4177,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 406.25
   },
   "source": "#TESTING\nx = np.random.random(size = (10, 5))\ny = np.random.randint(0,2,10)\nlog_reg = LogisticRegression(x,y)\nprint(\"\\n Accuracy:\", log_reg.accuracy, \"%\")",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "_Newtons Method Optimization_\n\nOptimizing...\n======================================================\nIteration: 1031\nAverage Absolute Beta-Step Difference: 0.0\nConverge: True\n\n_Optimization Terminated Successfully_\n_Optimized Betas_ [-1.1728053530724705, 5.958321865116244, -0.7608323589840151, -2.383416137878991, -2.353692168975133]\n\n Accuracy: 70.0 %\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Statsmodels** Implementation and Results.",
   "metadata": {
    "cell_id": "8ac43d1617ce40bfa91fef8eb50621d8",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "154206a26cf04907bb8eba08c791fdb9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c59b7247",
    "execution_start": 1649363818266,
    "execution_millis": 1426,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 244.75
   },
   "source": "# Compute logistic regression coefficients# using Newton’s method\nimport statsmodels.api as sm\nmodel = sm.Logit(y, x)\nresult = model.fit(method = \"newton\")\nprint(result.params)",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Optimization terminated successfully.\n         Current function value: 0.520427\n         Iterations 6\n[-1.17267262  5.95834742 -0.76087411 -2.38353518 -2.35370509]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=be83efb1-3870-4fac-886c-ad7be4823257' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "5824ebab-221a-4462-9dfc-258ec5db85bd",
  "deepnote_execution_queue": []
 }
}
